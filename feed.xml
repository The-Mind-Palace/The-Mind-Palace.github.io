<?xml version="1.0" encoding="utf-8"?><feed xmlns="http://www.w3.org/2005/Atom" xml:lang="en"><generator uri="https://jekyllrb.com/" version="4.3.4">Jekyll</generator><link href="https://the-mind-palace.github.io//feed.xml" rel="self" type="application/atom+xml" /><link href="https://the-mind-palace.github.io//" rel="alternate" type="text/html" hreflang="en" /><updated>2024-11-19T13:08:35+00:00</updated><id>https://the-mind-palace.github.io//feed.xml</id><title type="html">blank</title><entry><title type="html">Summary - ‘Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models’</title><link href="https://the-mind-palace.github.io//blog/2023/google-comm-paper-ASPLOS23-spring/" rel="alternate" type="text/html" title="Summary - ‘Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models’" /><published>2023-10-25T19:53:00+00:00</published><updated>2023-10-25T19:53:00+00:00</updated><id>https://the-mind-palace.github.io//blog/2023/google-comm-paper-ASPLOS23-spring</id><content type="html" xml:base="https://the-mind-palace.github.io//blog/2023/google-comm-paper-ASPLOS23-spring/"><![CDATA[<p><strong><em>Basic Idea</em></strong>  Overlap dependent communications and computations by starting compute on the half of compute inpute that has already been communicated</p>

<h2 id="motivation">Motivation</h2>

<p><img src="../images/google_comm_paper_ASPLOS23/image.png" alt="Alt text" /></p>

<p>LLMs require large on device memory. But memory capacity of a Single GPU or TPU is very limited.
To solve this people usually resort to intra-layer model parallelism. But this requires data communication across devices. And this communication contributes to a significant portion of the total execution time and severely hurts the compute efficiency.</p>

<p>The sequential behaviour of most deep learning models make it hard to overlap comm and compute that 
have data dependence.</p>

<h2 id="prior-solutions">Prior Solutions</h2>

<p>Applied various loop analysis and transformation techniques (such as loop fission, interchange, tiling, pipelining, strip-mining) to extract loops that only contain independent communication</p>

<h2 id="proposed--solution">Proposed  Solution</h2>

<p>The proposed solution uncovers opportunities for dependent communication compute overlap.  The original communication collective is decomposed into sequence of fine grained comm operations along with the dependent computation operation,[how fine grained? The data to be communicated and computed on is sharded]. It then proposes that these ops directly start working on each shard individually instead of waiting for all the units of the data are ready. It makes sure that there is never any data dependency violation.</p>

<p>In order to enable this, a new instruction scheduling scheme is proposed to asynchronously execute the decomposed collectives in parallel with the corresponding finer-grained computation ops in an iterative way</p>

<p>A cost model is also in place to assess the trade-offs of employing this optimization and automatically enables/disables it based on net benefits</p>

<p>The entire implementation is done on XLA, a TPU based ML compiler</p>

<h2 id="what-does-communication-mean-here">What does communication mean here?</h2>

<p>On a GPU, TP implementation normally looks like the following:</p>

<p><img src="../images/google_comm_paper_ASPLOS23/image-1.png" alt="Alt text" /></p>

<p>Here, f and g are conjugate. f is an identity operator in the forward pass and all reduce in the backward pass while g is an all reduce in the forward pass and identity in the backward pass.</p>

<p>This paper goes one step further and shards the activations and the weights. And construct them on demand using collective primitives.</p>

<p><img src="../images/google_comm_paper_ASPLOS23/image-2.png" alt="Alt text" /></p>

<p>Usually, usually the AllGather(y) X 2 is not required. Einsum here is just to imply matmul which is the primary operation of a linear layer</p>

<h3 id="communication-primitive">Communication Primitive</h3>

<p>• AllGather has a many-to-many communication pattern that each logical device partition receives data from all of the partitions and concatenate them based on the data Partition IDs. It is mostly applied to replicate data along partitioned dimensions
A good way to understand this is:</p>

<p><img src="../images/google_comm_paper_ASPLOS23/image-3.png" alt="Alt text" /></p>

<p>• Reduce Scatter  has a reverse communication pattern of AllGather. Each logical device partition performs element-wise reduction over the received data partitions with the same Partition ID.</p>

<p>• AllReduce can be considered as a ReduceScatter followed by an AllGather. After the operation, each logical device partition has the same tensor value from element-wise reduction over all of the data inputs, each of which comes from a different logical device partition.</p>

<p><img src="../images/google_comm_paper_ASPLOS23/image-4.png" alt="Alt text" /></p>

<p>[why Reduce Scatter + All Gather and not reduce + broadcast?. After a reduce, only one GPU [the root] has all the data. No einsum can start on the other ranks until the broadcast in which case the GPU is idle and there is no overlap to be had]</p>

<h2 id="what-does-deconstructed-communication-workflow-look-like">What does deconstructed communication workflow look like?</h2>

<p><img src="../images/google_comm_paper_ASPLOS23/image-5.png" alt="Alt text" /></p>

<p>Similarly,</p>

<p><img src="../images/google_comm_paper_ASPLOS23/image-6.png" alt="Alt text" /></p>

<p>C0 (the addition of C00 and C01) and C1 (the addition of C10 and C11)
are respectively the result shard of C after ReduceScatter on Partition 0 and Partition 1.</p>

<p>As the data communication is performed on the computation result, in this case, each device needs to asynchronously transfer the accumulation result shard rather than the
operand shard.</p>

<p>The accumulation result shard is initialized with zeros on each device. At the beginning of each iteration, each device asynchronously send the accumulation result shard (e.g., C0 and C1 on Partition 0 and Partition 1, respectively, in the first iteration) to the other device, and start executing the partial Einsum in parallel with the data transfer. [why communicate 0s? it’s wasted work :( ]</p>

<p>When the computation finishes, the partial
result is added to the received accumulation result shard at theend of the iteration (e.g., C1 += C10 and C0 += C01 on Partition 0
and Partition 1, respectively, in the first iteration).</p>

<p>But what is the overhead of overlapping communication and compute?</p>

<p><img src="../images/google_comm_paper_ASPLOS23/image-7.png" alt="Alt text" /></p>

<p>What happens to the compute time? Don’t know, TBD</p>

<h3 id="some-resources">Some Resources</h3>

<ol>
  <li>
    <p><a href="https://dl.acm.org/doi/pdf/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models, Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1</a></p>
  </li>
  <li>
    <p><a href="https://youtu.be/38TcJ9WE6vU">Lightening Talk at ASPLOS23</a></p>
  </li>
</ol>

<h2 id="references">References</h2>

<p>[1]  <a href="https://dl.acm.org/doi/pdf/10.1145/3567955.3567959">Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models, Proceedings of the 28th ACM International Conference on Architectural Support for Programming Languages and Operating Systems, Volume 1</a></p>

<p>[2]  <a href="https://stackoverflow.com/questions/15049190/difference-between-mpi-allgather-and-mpi-alltoall-functions">mpi - Difference between MPI_Allgather and MPI_Alltoall functions? - Stack Overflow</a></p>

<p>[3]  <a href="https://arxiv.org/pdf/1909.08053.pdf">Megatron-LM: Training Multi-Billion Parameter Language Models Using Model Parallelism (arxiv.org)</a></p>

<p>[4]  <a href="https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/operations.html#reduce">Operations — NCCL 2.6.4 documentation (nvidia.com)</a></p>

<p>[5] <a href="https://arxiv.org/pdf/2006.03318.pdf">Daydream: Accurately Estimating the Efficacy of Optimizations for DNN Training</a></p>]]></content><author><name></name></author><category term="systems-for-ml" /><summary type="html"><![CDATA[Summarizing what I understood from ASPLOS 2023's Overlap Communication with Dependent Computation via Decomposition in Large Deep Learning Models]]></summary></entry><entry><title type="html">What are CUDA Graphs?</title><link href="https://the-mind-palace.github.io//blog/2023/Intro-to-CUDA-Graphs/" rel="alternate" type="text/html" title="What are CUDA Graphs?" /><published>2023-10-24T19:53:00+00:00</published><updated>2023-10-24T19:53:00+00:00</updated><id>https://the-mind-palace.github.io//blog/2023/Intro-to-CUDA-Graphs</id><content type="html" xml:base="https://the-mind-palace.github.io//blog/2023/Intro-to-CUDA-Graphs/"><![CDATA[<p>With advancements in GPU hardware and software, CPU overhead in launching CUDA kernels is beginning to become a bottleneck. This is especially observed in many HPC applications as deep neural network training and scientific simulations. Workloads using Python instead of C++ also experience this bottleneck [compared to C++, Python is 5x slower].</p>

<p>And while there are a few solutions to this bottleneck, this post is about CUDA Graphs.</p>

<p><b> Table of Contents </b></p>

<h2 id="why-do-i-need-this">Why do I need this?</h2>

<p>For GPU kernels with short runtimes, CPU Launch time becomes an overhead. Separating out the definition of a graph from its execution reduces CPU kernel launch costs and can make a significant performance difference in such cases.</p>

<p>One can obviously make the case that this overhead can perhaps be removed by overlaping the launch of the kernel with the execution of another independent kernal. And while this is true, the overhead still exists, associated to multiple launches, as they would still require a separate launch operation for each kernel, where each is oblivious to the presence of the others.</p>

<p>Another means of solving this problem is to write fused kernels, but that is cumbersome.</p>

<p>CUDA Graphs have been designed to allow work to be defined as graphs rather than single operations. They address the above issue by providing a mechanism to launch multiple GPU operations through a single CPU operation, and hence reduce overheads</p>

<p><img src="../images/image_2_intro_to_CUDA.png" alt="" /></p>

<p>Graphs also enable the CUDA driver to perform a number of optimizations because the whole workflow is visible, including execution, data movement, and synchronization interactions, improving execution performance in a variety of cases (depending on the workload).</p>

<h2 id="what-is-this">What is this?</h2>

<p>CUDA Graphs give you a new manner of submitting your kernels using cuda.</p>

<p>CUDA operations form the nodes of a graph, with the edges being the dependencies between the operations. All CUDA work essentially forms a graph. This is realised by this data  abstraction.</p>

<p><img src="../images/image_1_intro_to_CUDA.png" alt="Fig: How CUDA Graphs are able to capture the workstreams' workflow" /></p>

<p>&lt;p style=”text-align: center;”Fig 2: How CUDA Graphs are able to capture the workstreams’ workflow*&lt;/p&gt;</p>

<p>The operations in the nodes can be:</p>

<ul>
  <li><strong>Kernel Launches</strong> :  <em>CUDA kernel running on GPU</em></li>
  <li><strong>CPU FUNCTION CALLS</strong> :  <em>Callback function on CPU</em></li>
  <li><strong>Memcopy/Memset</strong> :  <em>GPU data management</em></li>
  <li><strong>Memory Alloc/Free</strong> :  <em>Inline memory allocation</em></li>
  <li><strong>Sub-Graph</strong> :  <em>Graphs are hierarchical</em></li>
  <li><strong>empty node</strong></li>
  <li><strong>waiting on an event</strong></li>
  <li><strong>recording an event</strong></li>
  <li><strong>signalling an external semaphore</strong></li>
  <li><strong>waiting on an external semaphore</strong></li>
  <li><strong>conditional node</strong></li>
</ul>

<p>CUDA 12.3 introduced edge data on CUDA Graphs. Edge data modifies a dependency specified by an edge and consists of three parts: an outgoing port, an incoming port, and a type. An outgoing port specifies when an associated edge is triggered. An incoming port specifies what portion of a node is dependent on an associated edge. A type modifies the relation between the endpoints.</p>

<p>Port values are specific to node type and direction, and edge types may be restricted to specific node types. In all cases, zero-initialized edge data represents default behavior. Outgoing port 0 waits on an entire task, incoming port 0 blocks an entire task, and edge type 0 is associated with a full dependency with memory synchronizing behavior.</p>

<h2 id="how-do-i-use-this">How do I use this?</h2>

<p>To use CUDA Graphs:</p>

<ul>
  <li>
    <p><strong>Define a graph</strong> [<em>created in host code, Loaded from disk or built from libraries</em>]:</p>

    <p>During the definition phase, a program creates a description of the operations in the graph along with the dependencies between them.</p>
  </li>
  <li>
    <p><strong>Instantiate the graph</strong> [<em>Snapshot of template, Sets up &amp; initializes GPU execution structures, (create once, run many times)</em>]</p>

    <p>Instantiation takes a snapshot of the graph template, validates it, and performs much of the setup and initialization of work with the aim of minimizing what needs to be done at launch. The resulting instance is known as an executable graph.</p>
  </li>
  <li>
    <p><strong>Execute it</strong></p>

    <p>An executable graph may be launched into a stream, similar to any other CUDA work. It may be launched any number of times without repeating the instantiation.</p>
  </li>
</ul>

<p><img src="../images/image_3_intro_to_CUDA.png" alt="" /></p>

<p>A graph can be defined by either using StreamCapture mechanism or by defining the nodes and dependencies explicitly through newly available API calls. The Graphs may also span multiple GPUs.</p>

<p>Stream capture records asynchronous operations without actually launching a kernel. It follows inter-stream
dependencies to create forks &amp; joins. It can be used to construct a graph from normal CUDA stream syntax. It captures calls to external libraries. But if a library calls cudaStreamSynchronize() or any other synchronous operation, stream capture runs into a prolem. Since capture isn’t launching anything, synchronize cannot wait for anything and therefore capture fails.</p>

<p>The way around this is to explicitly create the graph using Graph APIs provided by CUDA or to create the graph using stream capture and then capture the library related node using the Graph API.</p>

<p><img src="../images/image_4_intro_to_CUDA.png" alt="" /></p>

<p>Graphs can be generated once and executed repeatedly. Data management may be optimized transparently through prefetching, read duplication, subdivision to finer granularity. Since Graphs capture cross-device dependencies, it can be used to optimise multi-device dependencies.
In situations where the workflow is not changing, the overhead of definition and instantiation can be amortized over many executions, and graphs provide a clear advantage over streams.</p>

<h3 id="updation-of-graphs">Updation of Graphs</h3>

<p>In situations where the workflow changes the graph becomes out of date and must be modified. Major changes to graph structure such as topology or types of nodes will require re-instantiation of the source graph because various topology-related optimization techniques must be re-applied.</p>

<p>The cost of repeated instantiation can reduce the overall performance benefit from graph execution, but it is common for only node parameters, such as kernel parameters and cudaMemcpy addresses, to change while graph topology remains the same.[ <strong>So topology can’t be updated :( . You have to redo the graph</strong>] For this case, CUDA provides a lightweight mechanism known as “Graph Update,” which allows certain node parameters to be modified in-place without having to rebuild the entire graph. This is much more efficient than re-instantiation.</p>

<p>CUDA also provides a mechanism for enabling and disabling individual nodes without affecting their current parameters.</p>

<p>CUDA provides two mechanisms for updating instantiated graph parameters, whole graph update and individual node update. Whole graph update allows the user to supply a topologically identical cudaGraph_t object whose nodes contain updated parameters. Individual node update allows the user to explicitly update the parameters of individual nodes. Using an updated cudaGraph_t is more convenient when a large number of nodes are being updated, or when the graph topology is unknown to the caller (i.e., The graph resulted from stream capture of a library call). Using individual node update is preferred when the number of changes is small and the user has the handles to the nodes requiring updates. Individual node update skips the topology checks and comparisons for unchanged nodes, so it can be more efficient in many cases.</p>

<p>However, there are some limitations to how certain graph nodes can and cannot be updated. For more details, please check out <a href="https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#cuda-graphs">this link</a>, it is a fantastice resource.</p>

<h2 id="device-graph-requirements">Device Graph Requirements</h2>

<p><strong>General requirements:</strong></p>

<ul>
  <li>The graph’s nodes must all reside on a single device.</li>
  <li>The graph can only contain kernel nodes, memcpy nodes, memset nodes, and child graph nodes.</li>
</ul>

<p><strong>Kernel nodes:</strong></p>

<ul>
  <li>Use of CUDA Dynamic Parallelism by kernels in the graph is not permitted.</li>
  <li>Cooperative launches are permitted so long as MPS is not in use.</li>
</ul>

<p><strong>Memcpy nodes:</strong></p>

<ul>
  <li>Only copies involving device memory and/or pinned device-mapped host memory are permitted.</li>
  <li>Copies involving CUDA arrays are not permitted.</li>
</ul>

<h2 id="nccl-support-for-cuda-graphs">NCCL SUPPORT FOR CUDA GRAPHS</h2>

<p>Recent releases have seen the adoption of CUDA Graphs for NCCL calls. NCCL enables GPU-based collective and P2P communications.</p>

<p>For distributed multi-GPU workloads, NCCL is used for collective communications. If we look at training a neural network that leverages data parallelism, without NCCL support for CUDA graphs, we’ll need a separate launch for each of forward/back propagation and NCCL AllReduce. By contrast, with NCCL support for CUDA graphs, we can reduce launch overhead by lumping together the forward/backward propagation and NCCL AllReduce all in a single graph launch. Starting with NCCL 2.11, when NCCL communication is captured and the CollNet algorithm is used, NCCL allows for further performance improvement via user buffer registration.</p>

<p>Having multiple outstanding NCCL operations that are any combination of graph-captured or non-captured is supported. There is a caveat that the mechanism NCCL uses internally to accomplish this has been seen to cause CUDA to deadlock when the graphs of multiple communicators are cudaGraphLaunch()’d from the same thread</p>

<p><img src="../images/image_5_intro_to_CUDA.png" alt="Alt text" /></p>

<h2 id="pytorch--cuda-graphs">PyTorch + CUDA Graphs</h2>

<p>PyTorch announced its adoption of CUDA Graph into advanced features to accelerate DL features.</p>

<p>PyTorch supports the construction of CUDA graphs using stream capture, which puts a CUDA stream in capture mode. CUDA work issued to a capturing stream doesn’t actually run on the GPU. Instead, the work is recorded in a graph. After capture, the graph can be launched to run the GPU work as many times as needed.</p>

<p>Each replay runs the same kernels with the same arguments. For pointer arguments this means the same memory addresses are used. By filling input memory with new data (e.g., from a new batch) before each replay, you can rerun the same work on new data.</p>

<p>A graph’s arguments and kernels are fixed, so a graph replay skips all layers of argument setup and kernel dispatch, including Python, C++, and CUDA driver overheads. Under the hood, a replay submits the entire graph’s work to the GPU with a single call to cudaGraphLaunch. Kernels in a replay also execute slightly faster on the GPU, but eliding CPU overhead is the main benefit.</p>

<p><strong>Constraints</strong></p>

<p>A set of ops is capturable if it doesn’t violate any of the following constraints:</p>

<ul>
  <li>Capture must occur on a non-default stream.</li>
  <li>Ops that synchronize the CPU with the GPU (e.g., .item() calls) are prohibited.</li>
  <li>CUDA RNG ops are allowed, but must use default generators. For example, explicitly constructing a new torch.Generator instance and passing it as the generator argument to an RNG function is prohibited.</li>
</ul>

<p><em>Violating any of the above will likely cause a runtime error</em></p>

<ul>
  <li>Within a process, only one capture may be underway at a time.</li>
  <li>No non-captured CUDA work may run in this process (on any thread) while capture is underway.</li>
  <li>CPU work is not captured. If the captured ops include CPU work, that work will be elided during replay.</li>
  <li>Every replay reads from and writes to the same (virtual) memory addresses.</li>
  <li>Dynamic control flow (based on CPU or GPU data) is prohibited.</li>
  <li>Dynamic shapes are prohibited. The graph assumes every tensor in the captured op sequence has the same size and layout in every replay.</li>
  <li>Using multiple streams in a capture is allowed, but there are restrictions.</li>
</ul>

<p><em>Violating any of the above will likely cause silent numerical errors or undefined behavior</em></p>

<p><strong>Non-constraints</strong></p>

<p>Once captured, the graph may be replayed on any stream.</p>

<h3 id="some-interesting-resources">Some interesting resources</h3>

<ul>
  <li>https://github.com/olcf/cuda-training-series/blob/master/exercises/hw13/README.md</li>
  <li>https://www.nvidia.com/en-us/on-demand/session/gtcspring21-s32082/</li>
  <li>https://github.com/NVIDIA/cuda-samples/tree/master/Samples/3_CUDA_Features/simpleCudaGraphs</li>
</ul>

<h2 id="references">References</h2>

<p>[1]  https://developer.nvidia.com/blog/cuda-10-features-revealed/</p>

<p>[2]  https://developer.nvidia.com/blog/cuda-graphs/</p>

<p>[3]  https://www.olcf.ornl.gov/wp-content/uploads/2021/10/013_CUDA_Graphs.pdf</p>

<p>[4]  https://docs.nvidia.com/cuda/cuda-c-programming-guide/index.html#graph-structure</p>

<p>[5]  https://docs.nvidia.com/deeplearning/nccl/user-guide/docs/usage/cudagraph.html</p>

<p>[6]  https://pytorch.org/blog/accelerating-pytorch-with-cuda-graphs/</p>

<p>[7] https://pytorch.org/docs/master/notes/cuda.html#cuda-graphs</p>]]></content><author><name></name></author><category term="systems-for-ml" /><summary type="html"><![CDATA[What are CUDA Graphs? Some things I learnt and some resources I loved]]></summary></entry></feed>