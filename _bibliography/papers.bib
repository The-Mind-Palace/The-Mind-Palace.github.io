---
@article{yang2023safe,
  bibtex_show={true},
  abbr={ICML},
  title={Better Safe than Sorry: Pre-training CLIP against Targeted Data Poisoning and Backdoor Attacks},
  abstract={Contrastive Language-Image Pre-training (CLIP) on large image-caption datasets has achieved remarkable success in zero-shot classification and enabled transferability to new domains. However, CLIP is extremely more vulnerable to targeted data poisoning and backdoor attacks, compared to supervised learning. Perhaps surprisingly, poisoning 0.0001% of CLIP pre-training data is enough to make targeted data poisoning attacks successful. This is four orders of magnitude smaller than what is required to poison supervised models. Despite this vulnerability, existing methods are very limited in defending CLIP models during pre-training. In this work, we propose a strong defense, SAFECLIP, to safely pre-train CLIP against targeted data poisoning and backdoor attacks. SAFECLIP warms up the model by applying unimodal contrastive learning (CL) on image and text modalities separately. Then, it divides the data into safe and risky sets, by applying a Gaussian Mixture Model to the cosine similarity of image-caption pair representations. SAFECLIP pre-trains the model by applying the CLIP loss to the safe set and applying unimodal CL to image and text modalities of the risky set separately. By gradually increasing the size of the safe set during pre-training, SAFECLIP effectively breaks targeted data poisoning and backdoor attacks without harming the CLIP performance. Our extensive experiments on CC3M, Visual Genome and MSCOCO demonstrate that SAFECLIP significantly reduces the success rate of targeted data poisoning attacks from 93.75% to 0% and that of various backdoor attacks from up to 100% to 0%, without harming CLIPâ€™s performance.},
  author={Yang., Wenhan and Gao, Jingdong and Mirzasoleiman, Baharan},
  journal={International Conference on Machine Learning (ICML)},
  pdf={yang24better.pdf},
  year={2024},
  code={https://github.com/BigML-CS-UCLA/SafeCLIP},
  poison={true}
}

@article{xue2024fewshot,
  bibtex_show={true},
  abbr={ICML},
  title={Few-shot Adaption to Distribution Shifts By Mixing Source and Target Embeddings},
  abstract={Pretrained machine learning models need to be adapted to distribution shifts when deployed in new target environments. When obtaining labeled data from the target distribution is expensive, few-shot adaptation with only a few examples from the target distribution becomes essential. In this work, we propose MixPro, a lightweight and highly data-efficient approach for few-shot adaptation. MixPro first generates a relatively large dataset by mixing (linearly combining) pre-trained embeddings of large source data with those of the few target examples. This process preserves important features of both source and target distributions, while mitigating the specific noise in the small target data. Then, it trains a linear classifier on the mixed embeddings to effectively adapts the model to the target distribution without overfitting the small target data. Theoretically, we demonstrate the advantages of MixPro over previous methods. Our experiments, conducted across various model architectures on 8 datasets featuring different types of distribution shifts, reveal that MixPro can outperform baselines by as much as 7%, with only 2-4 target examples.},
  author={Xue, Yihao and Payani, Ali and Yang, Yu and Mirzasoleiman, Baharan},
  journal={International Conference on Machine Learning (ICML)},
  pdf={xue24few.pdf},
  year={2024},
  spurious={true}
}

@article{lu2024newrf,
  bibtex_show={true},
  abbr={ICML},
  title={NeWRF: A Deep Learning Framework for Wireless Radiation Field Reconstruction and Channel Prediction},
  abstract={We present NeWRF, a deep learning framework for predicting wireless channels. Wireless channel prediction is a long-standing problem in the wireless community and is a key technology for improving the coverage of wireless network deployments. Today, a wireless deployment is evaluated by a site survey which is a cumbersome process requiring an experienced engineer to perform extensive channel measurements. To reduce the cost of site surveys, we develop NeWRF, which is based on recent advances in Neural Radiance Fields (NeRF). NeWRF trains a neural network model with a sparse set of channel measurements, and predicts the wireless channel accurately in any location in the site. We introduce a series of techniques that integrate wireless propagation properties into the NeWRF framework to account for the fundamental differences between the behavior of light and wireless signals. We conduct extensive evaluations of our framework and show that our approach can accurately predict channels at unvisited locations with significantly lower measurement density than the prior state-of-the-art.},
  author={Lu, Haofan and Vattheuer, Christopher and Mirzasoleiman, Baharan and Abari, Omid},
  journal={International Conference on Machine Learning (ICML)},
  pdf={lu24newrf.pdf},
  code={https://github.com/LuHaofan/NeWRF},
  year={2024}
}

@article{yang2023contrastive,
  bibtex_show={true},
  abbr={UAI},
  title={Graph Contrastive Learning under Heterophily via Graph Filters},
  abstract={Graph contrastive learning (CL) methods learn node representations in a self-supervised manner by maximizing the similarity between the augmented node representations obtained via a GNN-based encoder. However, CL methods perform poorly on graphs with heterophily, where connected nodes tend to belong to different classes. In this work, we address this problem by proposing an effective graph CL method, namely HLCL, for learning graph representations under heterophily. HLCL first identifies a homophilic and a heterophilic subgraph based on the cosine similarity of node features. It then uses a low-pass and a high-pass graph filter to aggregate representations of nodes connected in the homophilic subgraph and differentiate representations of nodes in the heterophilic subgraph. The final node representations are learned by contrasting both the augmented high-pass filtered views and the augmented low-pass filtered node views. Our extensive experiments show that HLCL outperforms state-ofthe-art graph CL methods on benchmark datasets with heterophily, as well as large-scale real-world graphs, by up to 7%, and outperforms graph supervised learning methods on datasets with heterophily by up to 10%.},
  author={Yang., Wenhan and Mirzasoleiman, Baharan},
  journal={Conference on Uncertainty in Artificial Intelligence (UAI)},
  pdf={yang24hlcl.pdf},
  code={https://github.com/BigML-CS-UCLA/HLCL},
  year={2024}
}

@article{xue2024final,
  bibtex_show={true},
  abbr={UAI},
  title={Investigating the Impact of Model Width and Density on Generalization in Presence of Label Noise},
  abstract={Increasing the size of overparameterized neural networks has been a key in achieving state-of-the-art performance. This is captured by the double descent phenomenon, where the test loss follows a decreasing-increasing-decreasing pattern (or sometimes monotonically decreasing) as model width increases. However, the effect of label noise on the test loss curve has not been fully explored. In this work, we uncover an intriguing phenomenon where label noise leads to a final ascent in the originally observed double descent curve. Specifically, under a sufficiently large noise-to-sample-size ratio, optimal generalization is achieved at intermediate widths. Through theoretical analysis, we attribute this phenomenon to the shape transition of test loss variance induced by label noise. Furthermore, we extend the final ascent phenomenon to model density and provide the first theoretical characterization showing that reducing density by randomly dropping trainable parameters improves generalization under label noise. We also thoroughly examine the roles of regularization and sample size. Surprisingly, we find that larger l2 regularization and robust learning methods against label noise exacerbate the final ascent. We confirm the validity of our findings through extensive experiments on ReLu networks trained on MNIST, ResNets/ViTs trained on CIFAR-10/100, and InceptionResNet-v2 trained on Stanford Cars with real-world noisy labels.},
  author={Xue, Yihao and Whitecross, Kyle and Mirzasoleiman, Baharan},
  journal={Conference on Uncertainty in Artificial Intelligence (UAI)},
  pdf={xue24final.pdf},
  supp={xue24final_long.pdf},
  year={2024},
  award={Spotlight presentation},
  noise={true}
}

@article{joshi2024data,
  bibtex_show={true},
  abbr={AISTATS},
  title={Data-Efficient Contrastive Language-Image Pretraining: Prioritizing Data Quality over Quantity},
  abstract={Contrastive Language-Image Pre-training (CLIP) on large-scale image-caption datasets learns representations that can achieve remarkable zero-shot generalization. However, such models require a massive amount of pre-training data. Improving the quality of the pre-training data has been shown to be much more effective in improving CLIP's performance than increasing its volume. Nevertheless, finding a subset of image-caption pairs that provably generalizes on par with the full data when trained on, has remained an open question. In this work, we propose the first theoretically rigorous data selection method for CLIP. We show that subsets that best preserve the cross-covariance of the images and captions of the full data best preserve CLIP's generalization performance. Our extensive experiments on ConceptualCaptions3M demonstrates that subsets of size 5%-10% found by ClipCov over 150% and 40% the accuracy of the next best baseline on ImageNet and its shifted versions. Moreover, we show that our subset exhibits average relative performance improvement over the next best baseline of nearly 50% across 14 downstream datasets.},
  author={Joshi, Siddharth and Jain, Arnav and Payani, Ali and Mirzasoleiman, Baharan},
  journal={International Conference on Artificial Intelligence and Statistics (AISTATS)},
  pdf={joshi24mmcl.pdf},
  supp={joshi24mmcl_long.pdf},
  year={2024},
  efficient={true}
}

---


@string{aps = {American Physical Society,}}


@article{prabhu2024vattentiondynamicmemorymanagement,
  bibtex_show={true},
  abbr={ArXiv},
  title={vAttention: Dynamic Memory Management for Serving LLMs without PagedAttention},
  abstract={Efficient management of GPU memory is essential for high throughput LLM inference. Prior systems used to reserve KV-cache memory ahead-of-time that resulted in wasted capacity due to internal fragmentation. Inspired by demand paging, vLLM proposed PagedAttention to enable dynamic memory allocation for KV-cache. This approach eliminates fragmentation and improves serving throughout. However, to be able to allocate physical memory dynamically, PagedAttention changes the layout of KV-cache from contiguous virtual memory to non-contiguous virtual memory. As a consequence, one needs to rewrite the attention kernels to support paging, and implement a memory manager in the serving framework. This results in both performance and programming overheads, as well as portability challenges in adopting state-of-the-art attention kernels. In this paper, we propose vAttention, a new approach for dynamic KV-cache memory management. In contrast to PagedAttention, vAttention stores KV-cache in contiguous virtual memory and leverages OS support for on-demand allocation of physical memory. vAttention thus enables one to use state-of-the art attention kernels out-of-the-box by adding support for dynamic allocation of physical memory without having to re-write their code. We implement vAttention in the vLLM serving stack to show that it also helps improve decode throughput by up to 1.99x over vLLM, and the end-to-end serving throughput by up to 1.22x and 1.29x, compared to using the state-of-the-art PagedAttention based kernels of FlashAttention and FlashInfer.},
  author={Prabhu, Ramya and Nayak, Ajay and Mohan, Jayashree and Ramjee, Ramachandran and Panwar, Ashish},
  journal={arXiv preprint arXiv:2405.04437},
  arxiv={2405.04437},
  year={Preprints}
}

